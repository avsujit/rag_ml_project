{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes and libraries from the Transformers library\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration, RagModel, RagSequenceForGeneration\n",
    "\n",
    "# Importing pandas library for working with data frames\n",
    "import pandas as pd\n",
    "\n",
    "# Importing numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Importing SentenceTransformer for creating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Importing Dataset class from datasets library\n",
    "from datasets import Dataset\n",
    "\n",
    "# Importing torch library for PyTorch operations\n",
    "import torch as torch\n",
    "\n",
    "# Importing specific classes and functions from datasets\n",
    "from datasets import Features, Sequence, Value, load_dataset\n",
    "\n",
    "# Importing partial function for creating partial functions\n",
    "from functools import partial\n",
    "\n",
    "# Importing List and Optional types from typing module\n",
    "from typing import List, Optional\n",
    "\n",
    "# Importing specific classes from Transformers library\n",
    "from transformers import (\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizerFast,\n",
    "    RagRetriever,\n",
    "    RagSequenceForGeneration,\n",
    "    RagTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a JSONL file(tot corpus jsonl file) into a Pandas DataFrame\n",
    "df = pd.read_json(\"./corpus.jsonl\", lines=True)\n",
    "\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns = [\"page_title\", \"text\", \"page_source\"]\n",
    "\n",
    "# Combine 'text' and 'page_source' columns into a single 'text' column\n",
    "df['text'] = df['text'].str.cat(df['page_source'], sep=' ')\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "df_t = df.loc[:, selected_columns]\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "new_df = df_t.copy()\n",
    "\n",
    "# Rename the 'page_title' column to 'title'\n",
    "new_df.rename(columns={'page_title': 'title'}, inplace=True)\n",
    "\n",
    "# Save the new DataFrame to a CSV file for ref\n",
    "new_df.to_csv(\"./my_knowledge_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf23136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, n=100, character=\" \") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the text every ``n``-th occurrence of ``character``.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to be split.\n",
    "        n (int): The number of occurrences before splitting.\n",
    "        character (str): The character to split the text on.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of split text passages.\n",
    "    \"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Split documents into passages.\n",
    "\n",
    "    Args:\n",
    "        documents (dict): A dictionary containing 'title' and 'text' keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'title' and 'text' keys containing split passages.\n",
    "    \"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(title + \" is the film name and its description is\" + passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "def custom_embeddings_in_batches(texts, titles, batch_size=8, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate embeddings for texts in batches using DPRContextEncoder.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of text passages.\n",
    "        titles (list): List of titles corresponding to each text passage.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        max_length (int): Maximum length of the input.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Concatenated embeddings of the input texts.\n",
    "    \"\"\"\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\").to(\"cuda\")\n",
    "    ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_titles = titles[i:i+batch_size]\n",
    "\n",
    "        input_ids = ctx_tokenizer(\n",
    "            batch_texts, padding=\"longest\", return_tensors=\"pt\", max_length=max_length, truncation=True\n",
    "        )[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = ctx_encoder(input_ids, return_dict=True).pooler_output.cpu().numpy()\n",
    "\n",
    "        embeddings_list.append(embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "# Split the documents into passages\n",
    "updated_cols = split_documents(new_df)\n",
    "\n",
    "# Create a new DataFrame with updated columns\n",
    "t_df = pd.DataFrame()\n",
    "t_df[\"title\"] = updated_cols[\"title\"]\n",
    "t_df[\"text\"] = updated_cols[\"text\"]\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate and save custom embeddings for the text passages\n",
    "embeddings = custom_embeddings_in_batches(t_df[\"text\"].values.tolist(), t_df[\"title\"].values.tolist(), batch_size=8)\n",
    "torch.save(embeddings, f\"./custom_embeddings.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_embeddings = torch.load(f\"./custom_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embeddings column to the DataFrame\n",
    "t_df['embeddings'] = embeddings.tolist()\n",
    "\n",
    "# Create a Dataset object from the Pandas DataFrame\n",
    "datasets_obj = Dataset.from_pandas(t_df)\n",
    "\n",
    "# Save the Dataset object to disk\n",
    "datasets_obj.save_to_disk('./dataset')\n",
    "\n",
    "# Add a FAISS index to the 'embeddings' column\n",
    "datasets_obj.add_faiss_index(\"embeddings\")\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "datasets_obj.get_index(\"embeddings\").save('./index.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088646f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RagRetriever from pretrained model\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    \"facebook/rag-sequence-nq\",\n",
    "    index_name=\"custom\",\n",
    "    passages_path=\"./dataset\",\n",
    "    index_path=\"./index.faiss\",\n",
    "    indexed_dataset=datasets_obj\n",
    ")\n",
    "\n",
    "# Initialize a RagTokenizer from pretrained model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "# Initialize a RagSequenceForGeneration from pretrained model\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input question using the tokenizer\n",
    "input_dict = tokenizer.question_encoder(\n",
    "    \"Movie from the early 2000s I believe about three people living in an apartment but never running into each other\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Move the model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate an answer using the model\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"])\n",
    "\n",
    "# Print the generated answer\n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c91b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Encode the input question using the tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mquestion_encoder(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname of a movie where a group of super heroes fighting together\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Move the model to GPU\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the input question using the tokenizer\n",
    "input_dict = tokenizer.question_encoder(\n",
    "    \"name of a movie where a group of super heroes fighting together\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Move the model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate an answer using the model\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"])\n",
    "\n",
    "\n",
    "# Print the generated answers\n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4abbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot",
   "language": "python",
   "name": "tot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
